{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительные задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=d18de14d920f37b77effa53265743cfcd11c1fd279c44fe22362c225006c91b5\n",
      "  Stored in directory: c:\\users\\olga.khamikoeva\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "#!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'ПИ19-3'\n",
    "s2 = 'ПМ19-3'\n",
    "edit_distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['высокопревосходительства',\n",
       " 'попреблагорассмотрительст',\n",
       " 'попреблагорассмотрительствующемуся',\n",
       " 'убегающих',\n",
       " 'уменьшившейся']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'велечайшим'\n",
    "with open('./data/litw-win.txt') as fp:\n",
    "    words = [line.strip().split()[-1] for line in fp]\n",
    "words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'величайшим'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(words, key=lambda w: edit_distance(w, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'попреблагорассмотрительств'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')\n",
    "word = 'попреблагорассмотрительствующемуся'\n",
    "stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'попреблагорассмотрительствующийся'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(word)[0].normalized.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Считайте слова из файла litw-win.txt и запишите их в список words.',\n",
       " 'В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words.',\n",
       " 'Считайте, что в слове есть опечатка, если данное слово не содержится в списке words']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words'''\n",
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer() #в первую очередь мы этой модели должны показать некоторые данные\n",
    "#CountV. просто возьмет и занумерует все слова\n",
    "cv.fit(sents)\n",
    "sent_cv = cv.transform(sents).toarray()\n",
    "sent_cv\n",
    "#строки матрицы разбиты по предложениям\n",
    "#1 значит, что слово с таким индексом присутствует в предложении\n",
    "#0 означает, что слова с таким индексом не было в предложении\n",
    "#2 означает, что слово с таким индексом встречалось два раза\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 35)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'считайте': 32,\n",
       " 'слова': 24,\n",
       " 'из': 12,\n",
       " 'файла': 33,\n",
       " 'litw': 0,\n",
       " 'win': 2,\n",
       " 'txt': 1,\n",
       " 'запишите': 11,\n",
       " 'их': 14,\n",
       " 'список': 31,\n",
       " 'words': 3,\n",
       " 'заданном': 9,\n",
       " 'предложении': 22,\n",
       " 'исправьте': 13,\n",
       " 'все': 5,\n",
       " 'опечатки': 21,\n",
       " 'заменив': 10,\n",
       " 'опечатками': 20,\n",
       " 'на': 16,\n",
       " 'ближайшие': 4,\n",
       " 'смысле': 27,\n",
       " 'расстояния': 23,\n",
       " 'левенштейна': 15,\n",
       " 'ним': 18,\n",
       " 'списка': 29,\n",
       " 'что': 34,\n",
       " 'слове': 25,\n",
       " 'есть': 8,\n",
       " 'опечатка': 19,\n",
       " 'если': 7,\n",
       " 'данное': 6,\n",
       " 'слово': 26,\n",
       " 'не': 17,\n",
       " 'содержится': 28,\n",
       " 'списке': 30}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import copy\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blanchard',\n",
       " 'southern',\n",
       " 'mirassou',\n",
       " 'seasonwhich',\n",
       " 'correctit',\n",
       " 'pleasureinduced',\n",
       " 'morsel',\n",
       " 'muchim',\n",
       " 'elecrtic',\n",
       " 'citrusonly',\n",
       " '26877',\n",
       " 'berriesraspberries',\n",
       " 'ruta',\n",
       " 'housewe',\n",
       " 'torgotta',\n",
       " 'presidents',\n",
       " 'episode',\n",
       " 'onepot',\n",
       " 'simmerman',\n",
       " 'reader',\n",
       " 'garces',\n",
       " 'wrongnote',\n",
       " 'chai',\n",
       " '43044',\n",
       " 'tequila',\n",
       " 'awesome',\n",
       " 'neighboring',\n",
       " 'sucks',\n",
       " 'veggis',\n",
       " 'wonderfulupdate',\n",
       " 'sincethey',\n",
       " 'nickey',\n",
       " 'appley',\n",
       " 'potine',\n",
       " 'elise',\n",
       " 'eachto',\n",
       " 'milkbuttermilk',\n",
       " 'neverfail',\n",
       " 'ruled',\n",
       " 'garota',\n",
       " 'dommmmmmmmm',\n",
       " 'sat',\n",
       " 'smooth',\n",
       " 'cosmo',\n",
       " 'vacation',\n",
       " 'christine',\n",
       " 'ramekin',\n",
       " 'solved',\n",
       " 'mincemeatmy',\n",
       " 'goodthese',\n",
       " 'tubebag',\n",
       " 'trove',\n",
       " 'mealnotes',\n",
       " 'garlicky',\n",
       " 'pepitas',\n",
       " 'ingredientsin',\n",
       " 'partysquick',\n",
       " 'crooning',\n",
       " 'gfree',\n",
       " 'achievement',\n",
       " 'driveinnplate',\n",
       " 'winelemonrosemary',\n",
       " 'sharing',\n",
       " 'wwwtarladalalcom',\n",
       " 'breeze',\n",
       " 'tsos',\n",
       " 'broken',\n",
       " 'deliciouslycreamy',\n",
       " 'crunchydone',\n",
       " 'journalconstitution',\n",
       " 'francethiswaycom',\n",
       " 'rightit',\n",
       " 'adusted',\n",
       " 'starspangled',\n",
       " 'training',\n",
       " 'preston',\n",
       " 'lofat',\n",
       " 'crunches',\n",
       " 'mixedgreens',\n",
       " 'description',\n",
       " 'greatprep',\n",
       " 'watermelonhalf',\n",
       " 'zanzibar',\n",
       " 'pate',\n",
       " 'lastest',\n",
       " 'shade',\n",
       " 'timesee',\n",
       " 'kaffee',\n",
       " 'refuel',\n",
       " 'nanners',\n",
       " 'vague',\n",
       " 'faidleys',\n",
       " 'whos',\n",
       " 'lowpriced',\n",
       " 'wflavor',\n",
       " 'jessicalee',\n",
       " 'mccormack',\n",
       " 'accounts',\n",
       " 'exboyfriends',\n",
       " 'tastyfor',\n",
       " 'frances',\n",
       " 'exclusions',\n",
       " 'duro',\n",
       " 'strings',\n",
       " 'uberhealthy',\n",
       " 'shulmans',\n",
       " 'nicole',\n",
       " 'submerging',\n",
       " 'fillyouup',\n",
       " 'stood',\n",
       " 'nutritionals',\n",
       " 'httpwwwyoutubecomwatchvkn3kj7hex0',\n",
       " 'bahamian',\n",
       " 'defrostreheat',\n",
       " 'sammy',\n",
       " '78579',\n",
       " 'souls',\n",
       " 'prepwork',\n",
       " 'hampered',\n",
       " 'sitei',\n",
       " 'yuck',\n",
       " 'garlic',\n",
       " 'emailing',\n",
       " 'khanom',\n",
       " 'bags',\n",
       " 'offbeat',\n",
       " 'television',\n",
       " 'kraft',\n",
       " 'annacias',\n",
       " 'hotadd',\n",
       " 'weil',\n",
       " 'tampered',\n",
       " 'mccormickmild',\n",
       " 'lauberge',\n",
       " 'mcdonald',\n",
       " 'quintessential',\n",
       " 'eggplant',\n",
       " 'georgetown',\n",
       " 'wwwwwrecipescom',\n",
       " 'potatos',\n",
       " 'stretchy',\n",
       " 'rudolph',\n",
       " 'methis',\n",
       " 'recipedont',\n",
       " 'gauteng',\n",
       " 'proscuito',\n",
       " 'ijust',\n",
       " 'wonderfulyou',\n",
       " 'huevos',\n",
       " 'sherry',\n",
       " 'infoper',\n",
       " 'grand',\n",
       " 'clarksdale',\n",
       " 'wis',\n",
       " 'toro',\n",
       " 'patoot',\n",
       " 'meanchefs',\n",
       " 'cathead',\n",
       " 'one800',\n",
       " 'clarifying',\n",
       " 'cornwall',\n",
       " 'phyos',\n",
       " 'greensboro',\n",
       " 'nuwaveflavorwave',\n",
       " 'hain',\n",
       " 'weggs',\n",
       " 'dory',\n",
       " 'chavrie',\n",
       " 'used',\n",
       " 'boullion',\n",
       " 'cookbooksome',\n",
       " 'hut',\n",
       " 'pomegranate',\n",
       " 'mee',\n",
       " 'riff',\n",
       " 'httpsourdoughhomecomblueberrymuffinshtmlnote',\n",
       " 'rollwiches',\n",
       " 'drab',\n",
       " 'caterpillar',\n",
       " 'bring',\n",
       " 'panic',\n",
       " 'minefrom',\n",
       " 'meateggplanttomato',\n",
       " 'looser',\n",
       " 'frankfurter',\n",
       " 'ras',\n",
       " 'pierre',\n",
       " 'hersheys',\n",
       " 'eyeopening',\n",
       " 'hodgepodge',\n",
       " 'cutnseal',\n",
       " 'price',\n",
       " 'recipezaar3',\n",
       " 'norns',\n",
       " 'royce',\n",
       " 'entice',\n",
       " 'volunteered',\n",
       " 'marvel',\n",
       " 'peaple',\n",
       " 'developed',\n",
       " 'execellent',\n",
       " 'gerhard',\n",
       " 'temperaure',\n",
       " 'shimoni',\n",
       " 'swift',\n",
       " 'hunts',\n",
       " 'freezes',\n",
       " '399',\n",
       " 'unfortunatelyi',\n",
       " 'dustins',\n",
       " 'properly',\n",
       " 'cookerwhile',\n",
       " '224',\n",
       " 'tastenote',\n",
       " 'calabrese',\n",
       " 'pillbury',\n",
       " 'muck',\n",
       " 'standalone',\n",
       " 'thickeners',\n",
       " 'pinkish',\n",
       " '4',\n",
       " 'population',\n",
       " 'disneyfamilys',\n",
       " 'sarasota',\n",
       " 'northeasternmidwestern',\n",
       " 'euphoric',\n",
       " 'oooh',\n",
       " 'convince',\n",
       " 'escoffier',\n",
       " 'increased',\n",
       " 'homeland',\n",
       " 'dissolve',\n",
       " 'right',\n",
       " 'blahness',\n",
       " 'asadero',\n",
       " '7inch',\n",
       " 'rollsadd',\n",
       " 'marries',\n",
       " 'quote',\n",
       " 'lil',\n",
       " 'grenada',\n",
       " 'gato',\n",
       " 'serendipity',\n",
       " 'simpletoprepare',\n",
       " 'allie',\n",
       " 'shores',\n",
       " 'gosh',\n",
       " 'wouldnt',\n",
       " '56g',\n",
       " 'band',\n",
       " 'postedchill',\n",
       " 'imperceptible',\n",
       " 'sheila',\n",
       " 'gingerglazed',\n",
       " 'wetter',\n",
       " 'drinkpinkvodkacom',\n",
       " 'streaks',\n",
       " 'kaspers',\n",
       " 'stereo',\n",
       " 'manhattan',\n",
       " '17g',\n",
       " 'dishhallacas',\n",
       " 'outdoorssomething',\n",
       " 'favoritegreat',\n",
       " 'connies',\n",
       " 'croque',\n",
       " 'powderjust',\n",
       " 'deliciousness',\n",
       " 'chocolatedeliveryvehicle',\n",
       " 'brombergs',\n",
       " 'mediumlarge',\n",
       " 'docs',\n",
       " 'goodi',\n",
       " 'gran',\n",
       " 'landed',\n",
       " 'forth',\n",
       " 'ingredient',\n",
       " 'butternut',\n",
       " 'pizzamy',\n",
       " 'rotation',\n",
       " 'athome',\n",
       " 'sucked',\n",
       " 'lennie',\n",
       " 'dressed',\n",
       " 'jus',\n",
       " 'finals',\n",
       " 'jorn',\n",
       " 'amysauntjo',\n",
       " 'frache',\n",
       " 'standing',\n",
       " 'bought',\n",
       " 'delcious',\n",
       " 'geema',\n",
       " 'foodi',\n",
       " 'fastif',\n",
       " 'alba',\n",
       " 'edgar',\n",
       " '80675',\n",
       " 'guanciale',\n",
       " 'snag',\n",
       " 'copycat',\n",
       " 'doorand',\n",
       " 'presegmented',\n",
       " 'taverns',\n",
       " 'davinci',\n",
       " 'carfabas',\n",
       " 'gi',\n",
       " 'skewed',\n",
       " 'mist',\n",
       " 'costamolino',\n",
       " 'esitmated',\n",
       " 'blackcherries',\n",
       " 'samenote',\n",
       " 'nyt',\n",
       " 'steve',\n",
       " 'moisest',\n",
       " 'mrwalkers',\n",
       " 'twinkled',\n",
       " 'wwwpuffpastrycom',\n",
       " 'shhhi',\n",
       " 'adjust',\n",
       " 'crisptender',\n",
       " '05',\n",
       " 'beansyes',\n",
       " 'lemonade',\n",
       " 'splendahalf',\n",
       " 'spurs',\n",
       " 'swapped',\n",
       " 'mcdonalds',\n",
       " 'lemonnut',\n",
       " '15minute',\n",
       " 'homedemo',\n",
       " 'blizzards',\n",
       " 'crustit',\n",
       " 'herein',\n",
       " 'intellectual',\n",
       " 'gasp',\n",
       " 'ellerbee',\n",
       " 'capsicums',\n",
       " 'cryeleike',\n",
       " 'seviche',\n",
       " 'microwave',\n",
       " 'husbandwho',\n",
       " 'made',\n",
       " 'timer',\n",
       " 'accommodating',\n",
       " 'omg',\n",
       " 'chefs',\n",
       " 'kicks',\n",
       " 'dawa',\n",
       " '112',\n",
       " 'fixins',\n",
       " 'crumbling',\n",
       " 'barista',\n",
       " 'sidedish',\n",
       " 'complementary',\n",
       " 'mataraza',\n",
       " 'spiced',\n",
       " 'custards',\n",
       " 'partake',\n",
       " 'studentchef',\n",
       " 'soupe',\n",
       " 'vegan',\n",
       " 'cans',\n",
       " 'sandwiched',\n",
       " 'feather',\n",
       " 'rotel',\n",
       " 'branch',\n",
       " '170293',\n",
       " 'noname',\n",
       " 'avocado',\n",
       " 'aarti',\n",
       " 'newtons',\n",
       " 'contests',\n",
       " 'fatnice',\n",
       " 'smattering',\n",
       " '173',\n",
       " 'vegsource',\n",
       " 'envisioned',\n",
       " 'hmmmm',\n",
       " 'comparatively',\n",
       " 'eat',\n",
       " 'cellaphane',\n",
       " 'strips',\n",
       " 'testers',\n",
       " 'helena',\n",
       " 'afternoons',\n",
       " 'vancouvers',\n",
       " 'julesongs',\n",
       " 'kiangsnaije',\n",
       " 'culturehttpwwwelanaspantrycombroccolirabewithgarlic',\n",
       " 'baciare',\n",
       " 'cares',\n",
       " 'gai',\n",
       " 'lehi',\n",
       " 'sprinkle',\n",
       " 'taiwanese',\n",
       " 'desterhazy',\n",
       " 'ceases',\n",
       " 'experimented',\n",
       " 'snacksfeel',\n",
       " 'punjab',\n",
       " 'bharadwaj',\n",
       " 'shopped',\n",
       " 'agreed',\n",
       " 'stirfried',\n",
       " 'secretshe',\n",
       " 'beansnutritional',\n",
       " '65721',\n",
       " 'descriptionserve',\n",
       " 'httpwwwetourscnchinacityguideyunnantravelguidediningkunmingfoodphp',\n",
       " '151',\n",
       " '209',\n",
       " 'transliterated',\n",
       " 'ommitted',\n",
       " '2pts',\n",
       " 'bourbonwhats',\n",
       " 'govenors',\n",
       " 'streusal',\n",
       " 'braise',\n",
       " 'moose',\n",
       " 'giving',\n",
       " 'teleras',\n",
       " '364735',\n",
       " 'gormet',\n",
       " 'indulgence',\n",
       " 'vesselsyou',\n",
       " 'quenchers',\n",
       " 'shulmani',\n",
       " 'upstate',\n",
       " 'breadthat',\n",
       " 'pharmacy',\n",
       " 'shoud',\n",
       " 'recipe266812',\n",
       " 'soba',\n",
       " 'irregular',\n",
       " 'bellagio',\n",
       " 'hug',\n",
       " '5year',\n",
       " 'africas',\n",
       " 'bounty',\n",
       " 'maturing',\n",
       " 'moister',\n",
       " 'crusty',\n",
       " 'alberta',\n",
       " 'soufflelike',\n",
       " 'depressed',\n",
       " 'limeadeit',\n",
       " '20817',\n",
       " 'kind',\n",
       " 'bharta',\n",
       " 'purchase',\n",
       " 'reward',\n",
       " 'revived',\n",
       " 'boilovers',\n",
       " 'exposed',\n",
       " 'homely',\n",
       " 'htis',\n",
       " 'robb',\n",
       " 'fulfills',\n",
       " 'jacobsons',\n",
       " 'caramelising',\n",
       " 'kinderlehrers',\n",
       " 'transfer',\n",
       " 'presents',\n",
       " 'donut',\n",
       " 'supersatisfy',\n",
       " 'lowcarbohydrate',\n",
       " 'partyserve',\n",
       " 'sudden',\n",
       " 'katsuobushidried',\n",
       " 'messing',\n",
       " 'bay',\n",
       " 'payards',\n",
       " 'rosti',\n",
       " 'currently',\n",
       " 'chewier',\n",
       " 'winndixie',\n",
       " 'else',\n",
       " 'ait',\n",
       " 'knocks',\n",
       " 'mingled',\n",
       " 'gcarbohydrates',\n",
       " 'oldtime',\n",
       " 'cheddar18',\n",
       " 'morse',\n",
       " 'nachos',\n",
       " 'removing',\n",
       " 'monicas',\n",
       " 'dressings',\n",
       " 'ncnicks',\n",
       " 'approachable',\n",
       " 'parmenter',\n",
       " 'clover',\n",
       " 'seniorsa',\n",
       " 'managers',\n",
       " 'comapny',\n",
       " 'fairlylowcarbohydrate',\n",
       " 'risetime',\n",
       " 'imperative',\n",
       " 'rustle',\n",
       " 'rewriting',\n",
       " 'montreal',\n",
       " 'congeniality',\n",
       " 'tightwe',\n",
       " 'brand',\n",
       " 'surprised',\n",
       " 'wishedyou',\n",
       " 'zrigschntzlets',\n",
       " 'residentshe',\n",
       " 'sense',\n",
       " 'nazi',\n",
       " 'snicks',\n",
       " 'bachs',\n",
       " '213mg',\n",
       " 'wellflavored',\n",
       " 'refrigerate',\n",
       " 'bcs',\n",
       " 'eatenthe',\n",
       " 'maghreb',\n",
       " '189040',\n",
       " 'doubters',\n",
       " 'girlhttpthepioneerwomancomcooking201210spinachartichokepasta',\n",
       " 'marg',\n",
       " 'today',\n",
       " 'lowanother',\n",
       " 'conservative',\n",
       " 'committed',\n",
       " 'unsalted',\n",
       " 'ginsburg',\n",
       " 'pluseach',\n",
       " 'ivillage',\n",
       " 'makeiteasier',\n",
       " 'pinit',\n",
       " 'broilersafe',\n",
       " 'tony',\n",
       " 'coatingspread',\n",
       " 'kauitzsch',\n",
       " 'littleshe',\n",
       " 'agios',\n",
       " 'verybestbakingcoms',\n",
       " 'continue',\n",
       " 'ymca',\n",
       " 'overeating',\n",
       " 'sauceyield',\n",
       " 'administration',\n",
       " 'omaha',\n",
       " 'cooklikeachampion',\n",
       " 'pastoral',\n",
       " 'cure',\n",
       " 'headed',\n",
       " 'cookbookthe',\n",
       " 'delicioustastes',\n",
       " 'virtualcities',\n",
       " 'superiority',\n",
       " 'owndont',\n",
       " 'yucky',\n",
       " 'aolioh',\n",
       " 'dube',\n",
       " 'emmantaler',\n",
       " 'allnew',\n",
       " 'ironrich',\n",
       " 'occupied',\n",
       " '44g',\n",
       " 'recipezaars',\n",
       " 'timethis',\n",
       " 'powerhouse',\n",
       " 'hocksqdm',\n",
       " 'shichimi',\n",
       " 'thuy',\n",
       " 'adopt',\n",
       " 'althernative',\n",
       " 'craisin',\n",
       " 'butterhorns',\n",
       " 'related',\n",
       " 'bachelor',\n",
       " 'singla',\n",
       " 'oooey',\n",
       " 'satisfies',\n",
       " 'loudon',\n",
       " 'rattray',\n",
       " 'leftoversveggies',\n",
       " 'draws',\n",
       " 'chokecherry',\n",
       " 'slab',\n",
       " 'sais',\n",
       " 'clearance',\n",
       " 'nauvooilthese',\n",
       " 'greens',\n",
       " 'picnicsthe',\n",
       " 'nursejaney',\n",
       " '3040',\n",
       " 'suburb',\n",
       " 'nips',\n",
       " 'oatcakes',\n",
       " 'sunshineyellow',\n",
       " 'tastessuper',\n",
       " 'saras',\n",
       " 'association',\n",
       " 'concauction',\n",
       " 'visted',\n",
       " 'penguins',\n",
       " 'lattes',\n",
       " 'parragon',\n",
       " 'grating',\n",
       " 'soakng',\n",
       " 'sheer',\n",
       " 'waitand',\n",
       " 'wwwfarmbuiltcom',\n",
       " 'blais',\n",
       " 'diyarbakir',\n",
       " 'mandatory',\n",
       " 'benedictines',\n",
       " 'knaus',\n",
       " 'cinch',\n",
       " 'whod',\n",
       " 'frenchs',\n",
       " 'alrighty',\n",
       " 'healty',\n",
       " 'donna',\n",
       " 'crimson',\n",
       " 'bradford',\n",
       " 'ingredientsamounts',\n",
       " 'practice',\n",
       " 'calzone',\n",
       " 'iw',\n",
       " 'waistlines',\n",
       " '32105',\n",
       " 'lowsalt',\n",
       " 'gardens102007',\n",
       " 'andaljucian',\n",
       " 'alden',\n",
       " 'slip',\n",
       " 'mozzarella',\n",
       " 'sageparmesan',\n",
       " 'deserved',\n",
       " 'medjool',\n",
       " '1800',\n",
       " 'nights',\n",
       " 'teaspoonful',\n",
       " 'oconner',\n",
       " 'absolutely',\n",
       " 'creamer',\n",
       " '482carbs',\n",
       " 'cafferty',\n",
       " 'zmail',\n",
       " 'spree',\n",
       " 'rebake',\n",
       " 'sunomono',\n",
       " 'pestolike',\n",
       " 'bodybuilding',\n",
       " 'nosugaradded',\n",
       " 'units',\n",
       " 'prozac',\n",
       " 'malnati',\n",
       " 'oni',\n",
       " 'webkinz',\n",
       " 'timeyour',\n",
       " 'crowning',\n",
       " 'marry',\n",
       " 'sats',\n",
       " 'bag',\n",
       " 'lasts',\n",
       " 'vahterzoy',\n",
       " 'bistro',\n",
       " 'homea',\n",
       " 'dishesbruschetta',\n",
       " 'highlight',\n",
       " 'tannins',\n",
       " 'hensperger',\n",
       " 'embarrassingly',\n",
       " 'pauls',\n",
       " '256550',\n",
       " 'lf',\n",
       " 'davidsons',\n",
       " 'odities',\n",
       " 'smell',\n",
       " 'ethiopian',\n",
       " 'recipegreen',\n",
       " 'emphasize',\n",
       " 'screwy',\n",
       " 'principles',\n",
       " 'wounds',\n",
       " 'horseshoe',\n",
       " 'dedicated',\n",
       " 'medditeranean',\n",
       " 'agohope',\n",
       " 'creole',\n",
       " 'banquet',\n",
       " '50mph',\n",
       " 'scrubbing',\n",
       " '1009january',\n",
       " 'phylloobia',\n",
       " 'concentrations',\n",
       " 'showed',\n",
       " 'cartons',\n",
       " 'pas',\n",
       " 'chopped1',\n",
       " 'memorized',\n",
       " 'vegetablei',\n",
       " 'headstart',\n",
       " 'alone',\n",
       " 'rises',\n",
       " 'maintaining',\n",
       " 'rustic',\n",
       " 'umph',\n",
       " 'deli',\n",
       " 'nowdays',\n",
       " 'gulash',\n",
       " 'california',\n",
       " 'ln',\n",
       " 'overdoing',\n",
       " 'cocktailsper',\n",
       " 'oversfreezes',\n",
       " 'revised',\n",
       " 'horrid',\n",
       " 'exquisitely',\n",
       " 'pensacola',\n",
       " 'courting',\n",
       " 'recipevery',\n",
       " 'chickeny',\n",
       " 'sir',\n",
       " 'bayless',\n",
       " 'christmasy',\n",
       " 'threshold',\n",
       " 'missing',\n",
       " 'teddys',\n",
       " 'door',\n",
       " 'thi',\n",
       " '81074',\n",
       " 'waferthin',\n",
       " 'offered',\n",
       " 'monkfishit',\n",
       " 'pantrys',\n",
       " 'lookingforwarmer',\n",
       " '24gfiber',\n",
       " 'appetitread',\n",
       " 'biphenl',\n",
       " 'realtors',\n",
       " 'event',\n",
       " 'ian',\n",
       " 'evelope',\n",
       " 'explained',\n",
       " 'reech',\n",
       " 'takeout',\n",
       " 'cuisinecomauby',\n",
       " '60g',\n",
       " 'fact',\n",
       " 'teri',\n",
       " 'nabisco',\n",
       " 'fruits',\n",
       " 'httpwwwclick10comhealth744266detailhtml',\n",
       " 'preferfrom',\n",
       " 'christian',\n",
       " 'matahami',\n",
       " 'wonders',\n",
       " 'sisinlaw',\n",
       " 'denneys',\n",
       " '212646',\n",
       " 'bolarong',\n",
       " 'winnerno',\n",
       " 'welltripling',\n",
       " 'warms',\n",
       " 'pam',\n",
       " 'benelux',\n",
       " '18t',\n",
       " 'arepas',\n",
       " 'pod',\n",
       " 'trek',\n",
       " 'bouillon',\n",
       " 'lovefeast',\n",
       " 'knack',\n",
       " 'bulgarian',\n",
       " 'tempo',\n",
       " 'pieceabout',\n",
       " 'barberries',\n",
       " 'considerable',\n",
       " '39lb',\n",
       " 'latvian',\n",
       " 'pocketless',\n",
       " 'hotasyouwantit',\n",
       " 'kgs',\n",
       " 'helens',\n",
       " 'sweetie',\n",
       " 'banish',\n",
       " 'gustafson',\n",
       " 'versatilewhy',\n",
       " 'uktv',\n",
       " 'economically',\n",
       " 'chicke',\n",
       " 'tad',\n",
       " 'treated',\n",
       " 'stop',\n",
       " 'grammy',\n",
       " 'rabbit',\n",
       " 'eclectic',\n",
       " 'promotional',\n",
       " 'pepperoni',\n",
       " 'mushiness',\n",
       " 'feijoada',\n",
       " 'celtic',\n",
       " 'dominicans',\n",
       " 'pamelas',\n",
       " 'basturma',\n",
       " 'reported',\n",
       " 'lineninner',\n",
       " 'oatmeal87173',\n",
       " 'writes',\n",
       " 'abita',\n",
       " 'improved',\n",
       " 'lent',\n",
       " 'morningso',\n",
       " 'assist',\n",
       " 'yumyou',\n",
       " 'smiththis',\n",
       " 'leanest',\n",
       " 'infamous',\n",
       " 'cortland',\n",
       " 'wholesomeespecially',\n",
       " 'kentucky',\n",
       " 'meze',\n",
       " 'salmonenjoy',\n",
       " 'bettycrockercom',\n",
       " 'overpowdering',\n",
       " 'spendid',\n",
       " 'tahoe',\n",
       " 'lammes',\n",
       " 'ot',\n",
       " 'wittie',\n",
       " 'surrounded',\n",
       " 'smoothness',\n",
       " '1106',\n",
       " 'robot',\n",
       " 'relate',\n",
       " 'colander',\n",
       " 'peanutbutter',\n",
       " 'commemorate',\n",
       " 'icefrost',\n",
       " 'moral',\n",
       " 'pounded',\n",
       " 'elliot',\n",
       " 'grana',\n",
       " '50s',\n",
       " 'pao',\n",
       " 'scoring',\n",
       " 'browniethe',\n",
       " '2nd',\n",
       " 'nad',\n",
       " 'choke',\n",
       " 'injected',\n",
       " '1824',\n",
       " 'porcupine',\n",
       " 'df',\n",
       " 'derangedmillionaire',\n",
       " 'worktry',\n",
       " 'therese',\n",
       " 'suffers',\n",
       " 'fil',\n",
       " 'hgtv',\n",
       " 'dissapear',\n",
       " 'havefried',\n",
       " 'huge',\n",
       " 'daikon',\n",
       " 'cornichons',\n",
       " 'membrane',\n",
       " 'ovenready',\n",
       " 'arlington',\n",
       " 'gruszecki',\n",
       " 'boiled',\n",
       " 'sauerkraut',\n",
       " 'easygood',\n",
       " 'claims',\n",
       " 'travelling',\n",
       " 'sinh',\n",
       " 'implements',\n",
       " 'choking',\n",
       " 'brag',\n",
       " 'abundance',\n",
       " 'thecooyon',\n",
       " 'hottasting',\n",
       " 'brothy',\n",
       " 'suggesting',\n",
       " 'whitewholewheatflour',\n",
       " 'groaned',\n",
       " 'cranks',\n",
       " 'lopez',\n",
       " 'flimsy',\n",
       " 'transforming',\n",
       " 'practical',\n",
       " 'voice',\n",
       " 'formerly',\n",
       " 'autumn',\n",
       " 'themto',\n",
       " 'peoplegujrati',\n",
       " '3star',\n",
       " 'oftenwe',\n",
       " 'igniting',\n",
       " 'wwwsplendacom',\n",
       " 'pancettai',\n",
       " 'consistencywise',\n",
       " 'track',\n",
       " 'cruise',\n",
       " 'gramma',\n",
       " 'era',\n",
       " 'nutsbring',\n",
       " 'organice',\n",
       " 'al',\n",
       " 'roast',\n",
       " 'eggit',\n",
       " 'troop',\n",
       " 'garliccumin',\n",
       " 'muminlaw',\n",
       " 'poohanpiglet',\n",
       " '365',\n",
       " 'outedited',\n",
       " 'recipe461319461319',\n",
       " 'freshmade',\n",
       " 'pyramid',\n",
       " 'connects',\n",
       " 'chubby',\n",
       " 'lamisons',\n",
       " 'aw',\n",
       " 'ingredienthow',\n",
       " 'greenwise',\n",
       " 'simpley',\n",
       " 'myselfsince',\n",
       " 'owen',\n",
       " 'brothersthis',\n",
       " 'tsuyu',\n",
       " 'thatand',\n",
       " 'ziti',\n",
       " '9x13inch',\n",
       " 'calle',\n",
       " 'levine',\n",
       " 'aims',\n",
       " 'lofland',\n",
       " 'panasonic',\n",
       " 'topp',\n",
       " 'seat',\n",
       " 'wellfrom',\n",
       " '64951',\n",
       " 'dfw',\n",
       " 'boling',\n",
       " 'fiber',\n",
       " 'lowno',\n",
       " 'nope',\n",
       " 'manpreet',\n",
       " 'pralines',\n",
       " 'yummmquick',\n",
       " 'shore',\n",
       " 'brasserie',\n",
       " 'brocolli',\n",
       " 'walton',\n",
       " 'trifles',\n",
       " 'kasseri',\n",
       " 'anise',\n",
       " 'created',\n",
       " 'inthis',\n",
       " 'anticipate',\n",
       " 'yemeni',\n",
       " 'lighthouse',\n",
       " 'chickenmushrooms',\n",
       " 'esp',\n",
       " 'workshavent',\n",
       " 'handed',\n",
       " 'tumblers',\n",
       " 'tarttangy',\n",
       " 'raiserthe',\n",
       " 'und',\n",
       " 'meltinginyourmouth',\n",
       " '711',\n",
       " 'huuuuuge',\n",
       " 'haleem',\n",
       " 'schwans',\n",
       " 'freeze',\n",
       " 'kwong',\n",
       " 'caramelizing',\n",
       " 'httpwwwstoufferscomproductsproductdetailsaspxproductid98servingsizeid0productgroupid0searchtextspinach20souffleservingsizeproductgroupcp1ispontrue',\n",
       " '37',\n",
       " 'ptsthanks',\n",
       " 'salad',\n",
       " 'specifics',\n",
       " 'recipeit',\n",
       " 'mask',\n",
       " 'cherryblack',\n",
       " 'scraping',\n",
       " 'phulka',\n",
       " 'likened',\n",
       " 'advantagesmore',\n",
       " 'transfatfree',\n",
       " 'points',\n",
       " 'change',\n",
       " 'moistdo',\n",
       " 'virus',\n",
       " 'soak',\n",
       " 'backyard',\n",
       " 'smallmedium',\n",
       " 'miserable',\n",
       " 'heavyduty',\n",
       " 'attacksso',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descrip = pd.read_csv(\"./data/preprocessed_descriptions.csv\")\n",
    "words = []\n",
    "for i in descrip[\"preprocessed_descriptions\"]:\n",
    "    words_i = nltk.word_tokenize(str(i))\n",
    "    for j in words_i:\n",
    "        words.append(j)\n",
    "words = list(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mugs', 'soupsatisfying'], ['morericeier', 'slices'], ['washcloth', 'carful'], ['wedges', 'cupfull'], ['recipelol', 'zaarian']]\n"
     ]
    }
   ],
   "source": [
    "exmpls = []\n",
    "for i in range(5):\n",
    "    pair = []\n",
    "    r_ind = random.randint(0, len(words)-1)\n",
    "    pair.append(words[r_ind])\n",
    "    r_ind = random.randint(0, len(words)-1)\n",
    "    pair.append(words[r_ind])\n",
    "    exmpls.append(pair)\n",
    "print(exmpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разница между словами \"mugs\" и \"soupsatisfying\" равна 12\n",
      "Разница между словами \"morericeier\" и \"slices\" равна 8\n",
      "Разница между словами \"washcloth\" и \"carful\" равна 7\n",
      "Разница между словами \"wedges\" и \"cupfull\" равна 7\n",
      "Разница между словами \"recipelol\" и \"zaarian\" равна 9\n"
     ]
    }
   ],
   "source": [
    "for i in exmpls:\n",
    "    print(f'Разница между словами \"{i[0]}\" и \"{i[1]}\" равна {edit_distance(i[0], i[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 наиболее близких слов к слову tajine:\n",
      "['tagine', 'taint', 'tahini']\n"
     ]
    }
   ],
   "source": [
    "def Levenstein_for_k(words, word, k):\n",
    "    examps = []\n",
    "    w_dist = {}\n",
    "    for i in words:\n",
    "        w_dist[i] = edit_distance(i, word)\n",
    "    all_w = copy.deepcopy(w_dist)\n",
    "    w_dist.pop(word)\n",
    "    while len(examps) < k:\n",
    "        min_len = min(w_dist, key=w_dist.get)\n",
    "        if min_len not in examps:\n",
    "            examps.append(min_len)\n",
    "            w_dist.pop(min_len)\n",
    "    print(f'{k} наиболее близких слов к слову {word}:\\n{examps}')\n",
    "    #print('Все слова: ', all_w) \n",
    "word = words[random.randint(0, len(words)-1)]\n",
    "Levenstein_for_k(words, word, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для лемматизации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>script</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>packetthe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>milkallergic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kinde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32863</th>\n",
       "      <td>overcooked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32864</th>\n",
       "      <td>tipskaffir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32865</th>\n",
       "      <td>prizewinning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32866</th>\n",
       "      <td>gd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32867</th>\n",
       "      <td>judgmenti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word stemmed_word normalized_word\n",
       "0            script          NaN             NaN\n",
       "1         packetthe          NaN             NaN\n",
       "2      milkallergic          NaN             NaN\n",
       "3             kinde          NaN             NaN\n",
       "4              ways          NaN             NaN\n",
       "...             ...          ...             ...\n",
       "32863    overcooked          NaN             NaN\n",
       "32864    tipskaffir          NaN             NaN\n",
       "32865  prizewinning          NaN             NaN\n",
       "32866            gd          NaN             NaN\n",
       "32867     judgmenti          NaN             NaN\n",
       "\n",
       "[32868 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2_1 = pd.DataFrame(columns = ['word', 'stemmed_word', 'normalized_word'])\n",
    "df_2_1['word'] = words\n",
    "df_2_1.set_index('word')\n",
    "df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>script</td>\n",
       "      <td>script</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>packetthe</td>\n",
       "      <td>packetth</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>milkallergic</td>\n",
       "      <td>milkallerg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kinde</td>\n",
       "      <td>kind</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ways</td>\n",
       "      <td>way</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32863</th>\n",
       "      <td>overcooked</td>\n",
       "      <td>overcook</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32864</th>\n",
       "      <td>tipskaffir</td>\n",
       "      <td>tipskaffir</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32865</th>\n",
       "      <td>prizewinning</td>\n",
       "      <td>prizewin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32866</th>\n",
       "      <td>gd</td>\n",
       "      <td>gd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32867</th>\n",
       "      <td>judgmenti</td>\n",
       "      <td>judgmenti</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word stemmed_word normalized_word\n",
       "0            script       script             NaN\n",
       "1         packetthe     packetth             NaN\n",
       "2      milkallergic   milkallerg             NaN\n",
       "3             kinde         kind             NaN\n",
       "4              ways          way             NaN\n",
       "...             ...          ...             ...\n",
       "32863    overcooked     overcook             NaN\n",
       "32864    tipskaffir   tipskaffir             NaN\n",
       "32865  prizewinning     prizewin             NaN\n",
       "32866            gd           gd             NaN\n",
       "32867     judgmenti    judgmenti             NaN\n",
       "\n",
       "[32868 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemm_w = []\n",
    "for word in df_2_1['word']:\n",
    "    stemm_w.append(stemmer.stem(word))\n",
    "df_2_1['stemmed_word'] = stemm_w\n",
    "df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\olga.khamikoeva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>script</td>\n",
       "      <td>script</td>\n",
       "      <td>script</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>packetthe</td>\n",
       "      <td>packetth</td>\n",
       "      <td>packetthe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>milkallergic</td>\n",
       "      <td>milkallerg</td>\n",
       "      <td>milkallergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kinde</td>\n",
       "      <td>kind</td>\n",
       "      <td>kinde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ways</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32863</th>\n",
       "      <td>overcooked</td>\n",
       "      <td>overcook</td>\n",
       "      <td>overcooked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32864</th>\n",
       "      <td>tipskaffir</td>\n",
       "      <td>tipskaffir</td>\n",
       "      <td>tipskaffir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32865</th>\n",
       "      <td>prizewinning</td>\n",
       "      <td>prizewin</td>\n",
       "      <td>prizewinning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32866</th>\n",
       "      <td>gd</td>\n",
       "      <td>gd</td>\n",
       "      <td>gd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32867</th>\n",
       "      <td>judgmenti</td>\n",
       "      <td>judgmenti</td>\n",
       "      <td>judgmenti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32868 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word stemmed_word normalized_word\n",
       "0            script       script          script\n",
       "1         packetthe     packetth       packetthe\n",
       "2      milkallergic   milkallerg    milkallergic\n",
       "3             kinde         kind           kinde\n",
       "4              ways          way             way\n",
       "...             ...          ...             ...\n",
       "32863    overcooked     overcook      overcooked\n",
       "32864    tipskaffir   tipskaffir      tipskaffir\n",
       "32865  prizewinning     prizewin    prizewinning\n",
       "32866            gd           gd              gd\n",
       "32867     judgmenti    judgmenti       judgmenti\n",
       "\n",
       "[32868 rows x 3 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "wnl_list = []\n",
    "for word in df_2_1['word']:\n",
    "    new_w = wnl.lemmatize(word)\n",
    "    wnl_list.append(new_w)\n",
    "df_2_1['normalized_word'] = wnl_list\n",
    "df_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\olga.khamikoeva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>new_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ind</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "      <td>original recipe created chef scott meskan geor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "      <td>children friends ask homemade popsicles mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "      <td>go surprised even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "      <td>sisterinlaw made us family get together delici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "      <td>think fondue romantic casual dinner wonderful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29364</th>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "      <td>based french recipe changed substantially warn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29365</th>\n",
       "      <td>zwetschgenkuchen  bavarian plum cake</td>\n",
       "      <td>this is a traditional fresh plum cake thought ...</td>\n",
       "      <td>traditional fresh plum cake thought originated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29366</th>\n",
       "      <td>zwiebelkuchen   southwest german onion cake</td>\n",
       "      <td>this is a traditional late summer early fall s...</td>\n",
       "      <td>traditional late summer early fall snack usual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29367</th>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>this is a delicious soup that i originally fou...</td>\n",
       "      <td>delicious soup originally found better homes g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29368</th>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>ive heard of the cookies by design company but...</td>\n",
       "      <td>ive heard cookies design company never tried c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29369 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "ind                                                   \n",
       "0             george s at the cove  black bean soup   \n",
       "1                healthy for them  yogurt popsicles   \n",
       "2                      i can t believe it s spinach   \n",
       "3                              italian  gut busters   \n",
       "4          love is in the air  beef fondue   sauces   \n",
       "...                                             ...   \n",
       "29364  zurie s holey rustic olive and cheddar bread   \n",
       "29365          zwetschgenkuchen  bavarian plum cake   \n",
       "29366   zwiebelkuchen   southwest german onion cake   \n",
       "29367                                   zydeco soup   \n",
       "29368        cookies by design   cookies on a stick   \n",
       "\n",
       "                                             description  \\\n",
       "ind                                                        \n",
       "0      an original recipe created by chef scott meska...   \n",
       "1      my children and their friends ask for my homem...   \n",
       "2                  these were so go it surprised even me   \n",
       "3      my sisterinlaw made these for us at a family g...   \n",
       "4      i think a fondue is a very romantic casual din...   \n",
       "...                                                  ...   \n",
       "29364  this is based on a french recipe but i changed...   \n",
       "29365  this is a traditional fresh plum cake thought ...   \n",
       "29366  this is a traditional late summer early fall s...   \n",
       "29367  this is a delicious soup that i originally fou...   \n",
       "29368  ive heard of the cookies by design company but...   \n",
       "\n",
       "                                         new_description  \n",
       "ind                                                       \n",
       "0      original recipe created chef scott meskan geor...  \n",
       "1      children friends ask homemade popsicles mornin...  \n",
       "2                                      go surprised even  \n",
       "3      sisterinlaw made us family get together delici...  \n",
       "4      think fondue romantic casual dinner wonderful ...  \n",
       "...                                                  ...  \n",
       "29364  based french recipe changed substantially warn...  \n",
       "29365  traditional fresh plum cake thought originated...  \n",
       "29366  traditional late summer early fall snack usual...  \n",
       "29367  delicious soup originally found better homes g...  \n",
       "29368  ive heard cookies design company never tried c...  \n",
       "\n",
       "[29369 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2_2 = pd.DataFrame(columns = ['name', 'description', 'new_description'])\n",
    "df_2_2['name'] = descrip['name']\n",
    "df_2_2['description'] = descrip['preprocessed_descriptions']\n",
    "df_2_2 = df_2_2[pd.notna(descrip[\"preprocessed_descriptions\"])]\n",
    "\n",
    "new_desc = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for desc in df_2_2['description']:\n",
    "    desc_tokens = nltk.word_tokenize(desc)\n",
    "    tokens_without_sw = [word for word in desc_tokens if not word in stop_words]\n",
    "    a = ' '.join(tokens_without_sw)\n",
    "    new_desc.append(a)\n",
    "df_2_2['new_description'] = new_desc\n",
    "\n",
    "df_2_2.reset_index()\n",
    "df_2_2['ind'] = list(range(0, df_2_2.shape[0]))\n",
    "df_2_2.set_index('ind', inplace=True)\n",
    "df_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля стоп-слов составляет 47%.\n"
     ]
    }
   ],
   "source": [
    "all_string = []\n",
    "all_words = []\n",
    "\n",
    "filt_string = []\n",
    "filt_words = []\n",
    "\n",
    "for i in df_2_2[\"description\"].fillna(\"\"):\n",
    "    all_string.append(i)\n",
    "all_words = nltk.word_tokenize(''.join(all_string))\n",
    "\n",
    "for i in df_2_2[\"new_description\"].fillna(\"\"):\n",
    "    filt_string.append(i)\n",
    "filt_words = nltk.word_tokenize(''.join(filt_string))\n",
    "\n",
    "print(f'Доля стоп-слов составляет {round((len(all_words)-len(filt_words))/len(all_words)*100)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(all_words)\n",
    "fdist2 = FreqDist(filt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 самых часто употребляемых слов до удаления стоп-слов:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 39495),\n",
       " ('a', 32404),\n",
       " ('and', 30238),\n",
       " ('to', 23421),\n",
       " ('i', 21419),\n",
       " ('is', 20239),\n",
       " ('this', 19292),\n",
       " ('it', 18980),\n",
       " ('of', 18357),\n",
       " ('for', 15772)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Топ-10 самых часто употребляемых слов до удаления стоп-слов:')\n",
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 самых часто употребляемых слов после удаления стоп-слов:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('recipe', 12257),\n",
       " ('make', 5899),\n",
       " ('use', 4451),\n",
       " ('time', 4307),\n",
       " ('like', 3696),\n",
       " ('made', 3469),\n",
       " ('great', 3403),\n",
       " ('one', 3350),\n",
       " ('easy', 3286),\n",
       " ('good', 3078)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Топ-10 самых часто употребляемых слов после удаления стоп-слов:')\n",
    "fdist2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10490</th>\n",
       "      <td>10490</td>\n",
       "      <td>elaine s lasagna</td>\n",
       "      <td>my moms delicious lasagna recipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21688</th>\n",
       "      <td>21688</td>\n",
       "      <td>pumpkin cupcakes with kahlua cream cheese fros...</td>\n",
       "      <td>i bake these cupcakes every year for halloween...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6191</th>\n",
       "      <td>6191</td>\n",
       "      <td>chile jam chicken</td>\n",
       "      <td>an easy and delicious chicken recipe the chile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10876</th>\n",
       "      <td>10876</td>\n",
       "      <td>fennel   fontina cheese sandwich</td>\n",
       "      <td>i love fennel and fontina  a different and tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>4100</td>\n",
       "      <td>bulls eye salad</td>\n",
       "      <td>i cant come up with a good name for this salad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               name  \\\n",
       "10490       10490                                   elaine s lasagna   \n",
       "21688       21688  pumpkin cupcakes with kahlua cream cheese fros...   \n",
       "6191         6191                                  chile jam chicken   \n",
       "10876       10876                   fennel   fontina cheese sandwich   \n",
       "4100         4100                                    bulls eye salad   \n",
       "\n",
       "                               preprocessed_descriptions  \n",
       "10490                   my moms delicious lasagna recipe  \n",
       "21688  i bake these cupcakes every year for halloween...  \n",
       "6191   an easy and delicious chicken recipe the chile...  \n",
       "10876  i love fennel and fontina  a different and tas...  \n",
       "4100   i cant come up with a good name for this salad...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes = pd.read_csv(\"./data/preprocessed_descriptions.csv\")\n",
    "rec_5 = recipes.dropna().sample(5)\n",
    "rec_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.39346994, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.58752141, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.58752141,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.39346994, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]]),\n",
       " array([[0.        , 0.23939814, 0.        , 0.23939814, 0.        ,\n",
       "         0.23939814, 0.        , 0.23939814, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.23939814,\n",
       "         0.23939814, 0.        , 0.16032773, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.23939814,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.23939814,\n",
       "         0.23939814, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.47879629, 0.        , 0.        , 0.23939814, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.23939814,\n",
       "         0.23939814, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.23939814, 0.        , 0.        ]]),\n",
       " array([[0.14694403, 0.        , 0.14694403, 0.        , 0.14694403,\n",
       "         0.        , 0.4408321 , 0.        , 0.14694403, 0.14694403,\n",
       "         0.        , 0.14694403, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.14694403, 0.09841013, 0.        , 0.14694403,\n",
       "         0.14694403, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14694403, 0.        ,\n",
       "         0.        , 0.        , 0.14694403, 0.        , 0.14694403,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.11855352, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.14694403, 0.14694403, 0.14694403, 0.14694403,\n",
       "         0.14694403, 0.49205066, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.29388807, 0.        ,\n",
       "         0.        , 0.14694403, 0.14694403, 0.14694403, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14694403, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]]),\n",
       " array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.33150716, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.33150716, 0.        ,\n",
       "         0.        , 0.33150716, 0.        , 0.33150716, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.33150716, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.26745789, 0.33150716, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.22201421, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.33150716, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.33150716,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]]),\n",
       " array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.16666667, 0.        , 0.        , 0.16666667, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.16666667, 0.        , 0.        ,\n",
       "         0.16666667, 0.16666667, 0.16666667, 0.        , 0.        ,\n",
       "         0.        , 0.16666667, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.16666667, 0.        , 0.        , 0.16666667,\n",
       "         0.        , 0.        , 0.16666667, 0.16666667, 0.        ,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.16666667, 0.16666667, 0.16666667, 0.        , 0.16666667,\n",
       "         0.16666667, 0.16666667, 0.        , 0.33333333, 0.16666667]])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=\"english\")\n",
    "vectorizer.fit(rec_5[\"preprocessed_descriptions\"])\n",
    "vector = []\n",
    "for i in rec_5[\"preprocessed_descriptions\"]:\n",
    "    vector.append(vectorizer.transform([i]).toarray())\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 75)\n",
      "(1, 75)\n",
      "(1, 75)\n",
      "(1, 75)\n",
      "(1, 75)\n"
     ]
    }
   ],
   "source": [
    "desc1 = vector[0]\n",
    "desc2 = vector[1]\n",
    "desc3 = vector[2]\n",
    "desc4 = vector[3]\n",
    "desc5 = vector[4]\n",
    "print(desc1.shape)\n",
    "print(desc2.shape)\n",
    "print(desc3.shape)\n",
    "print(desc4.shape)\n",
    "print(desc5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elaine s lasagna</th>\n",
       "      <th>pumpkin cupcakes with kahlua cream cheese frosting</th>\n",
       "      <th>chile jam chicken</th>\n",
       "      <th>fennel   fontina cheese sandwich</th>\n",
       "      <th>bulls eye salad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elaine s lasagna</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpkin cupcakes with kahlua cream cheese frosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chile jam chicken</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fennel   fontina cheese sandwich</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bulls eye salad</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   elaine s lasagna  \\\n",
       "names                                                                 \n",
       "elaine s lasagna                                                NaN   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting              NaN   \n",
       "chile jam chicken                                               NaN   \n",
       "fennel   fontina cheese sandwich                                NaN   \n",
       "bulls eye salad                                                 NaN   \n",
       "\n",
       "                                                   pumpkin cupcakes with kahlua cream cheese frosting  \\\n",
       "names                                                                                                   \n",
       "elaine s lasagna                                                                                  NaN   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting                                                NaN   \n",
       "chile jam chicken                                                                                 NaN   \n",
       "fennel   fontina cheese sandwich                                                                  NaN   \n",
       "bulls eye salad                                                                                   NaN   \n",
       "\n",
       "                                                   chile jam chicken  \\\n",
       "names                                                                  \n",
       "elaine s lasagna                                                 NaN   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting               NaN   \n",
       "chile jam chicken                                                NaN   \n",
       "fennel   fontina cheese sandwich                                 NaN   \n",
       "bulls eye salad                                                  NaN   \n",
       "\n",
       "                                                   fennel   fontina cheese sandwich  \\\n",
       "names                                                                                 \n",
       "elaine s lasagna                                                                NaN   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting                              NaN   \n",
       "chile jam chicken                                                               NaN   \n",
       "fennel   fontina cheese sandwich                                                NaN   \n",
       "bulls eye salad                                                                 NaN   \n",
       "\n",
       "                                                   bulls eye salad  \n",
       "names                                                               \n",
       "elaine s lasagna                                               NaN  \n",
       "pumpkin cupcakes with kahlua cream cheese frosting             NaN  \n",
       "chile jam chicken                                              NaN  \n",
       "fennel   fontina cheese sandwich                               NaN  \n",
       "bulls eye salad                                                NaN  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for i in rec_5['name']:\n",
    "    names.append(i)\n",
    "df_3_1 = pd.DataFrame(columns = [names[0], names[1], names[2], names[3], names[4]])\n",
    "df_3_1['names'] = names\n",
    "df_3_1.set_index('names',inplace=True)\n",
    "df_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elaine s lasagna</th>\n",
       "      <th>pumpkin cupcakes with kahlua cream cheese frosting</th>\n",
       "      <th>chile jam chicken</th>\n",
       "      <th>fennel   fontina cheese sandwich</th>\n",
       "      <th>bulls eye salad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elaine s lasagna</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936916</td>\n",
       "      <td>0.767671</td>\n",
       "      <td>0.912644</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pumpkin cupcakes with kahlua cream cheese frosting</th>\n",
       "      <td>0.936916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chile jam chicken</th>\n",
       "      <td>0.767671</td>\n",
       "      <td>0.984222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fennel   fontina cheese sandwich</th>\n",
       "      <td>0.912644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bulls eye salad</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    elaine s lasagna  \\\n",
       "names                                                                  \n",
       "elaine s lasagna                                            0.000000   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting          0.936916   \n",
       "chile jam chicken                                           0.767671   \n",
       "fennel   fontina cheese sandwich                            0.912644   \n",
       "bulls eye salad                                             1.000000   \n",
       "\n",
       "                                                    pumpkin cupcakes with kahlua cream cheese frosting  \\\n",
       "names                                                                                                    \n",
       "elaine s lasagna                                                                             0.936916    \n",
       "pumpkin cupcakes with kahlua cream cheese frosting                                           0.000000    \n",
       "chile jam chicken                                                                            0.984222    \n",
       "fennel   fontina cheese sandwich                                                             1.000000    \n",
       "bulls eye salad                                                                              1.000000    \n",
       "\n",
       "                                                    chile jam chicken  \\\n",
       "names                                                                   \n",
       "elaine s lasagna                                             0.767671   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting           0.984222   \n",
       "chile jam chicken                                            0.000000   \n",
       "fennel   fontina cheese sandwich                             0.859050   \n",
       "bulls eye salad                                              1.000000   \n",
       "\n",
       "                                                    fennel   fontina cheese sandwich  \\\n",
       "names                                                                                  \n",
       "elaine s lasagna                                                            0.912644   \n",
       "pumpkin cupcakes with kahlua cream cheese frosting                          1.000000   \n",
       "chile jam chicken                                                           0.859050   \n",
       "fennel   fontina cheese sandwich                                            0.000000   \n",
       "bulls eye salad                                                             1.000000   \n",
       "\n",
       "                                                    bulls eye salad  \n",
       "names                                                                \n",
       "elaine s lasagna                                                1.0  \n",
       "pumpkin cupcakes with kahlua cream cheese frosting              1.0  \n",
       "chile jam chicken                                               1.0  \n",
       "fennel   fontina cheese sandwich                                1.0  \n",
       "bulls eye salad                                                 0.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df_3_1)):     \n",
    "    dist = []\n",
    "    for j in range(len(df_3_1)):\n",
    "        dist.append(distance.cosine(vector[i], vector[j]))\n",
    "    df_3_1[names[i]] = dist\n",
    "df_3_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы ответить на этот вопрос, нужно понять некоторые факты:\n",
    "* 0 означает, что рецепты идентичны\n",
    "* 1 означает, что рецепты совершенно различны\n",
    "Таким образом, что чем ближе к 0 значение близости между парой рецептов, тем более похожими они являются.\n",
    "Рассмотрев внимательно датафрейм, можно сделать вывод, что топ-2 наиболее похожих рецептов:\n",
    "1. Пара \"chile jam chicken\" и \"elaine s lasagna\" (0.767671) (строка 3 столбец 1)\n",
    "2. Пара \"fennel fontina cheese sandwich\" и \"chile jam chicken\" (0.859050) (строка 4 столбец 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
